<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <title>Tic-Tac-Toe</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
    <style>
        #board {
            display: grid;
            grid-template-columns: repeat(3, 104px);
            grid-gap: 1px;
            margin: 50px auto;
            width: max-content;
        }

        .cell {
            width: 100px;
            height: 100px;
            font-size: 2em;
            cursor: pointer;
            border: 2px solid #333;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #message {
            font-size: 1.5em;
            margin-top: 20px;
        }

        button {
            margin-top: 20px;
            padding: 10px 20px;
            font-size: 1em;
        }

        #gameMode {
            padding: 5px 10px;
            font-size: 1em;
            margin-left: 10px;
        }
    </style>
</head>

<body>
    <a href="../index.html" title="Back to Home" class="home-link">
        <svg width="28" height="28" viewBox="0 0 24 24">
            <path d="M15 18l-6-6 6-6" stroke="#333" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/>
        </svg>
        <span>Home</span>
    </a>
    <h1>Tic-Tac-Toe</h1>
    <div id="game-container">
        <div id="game">
            <label for="gameMode">Game Mode:</label>
            <select id="gameMode" onchange="updateGameMode()">
                <option selected value="random">Random Bot (Easy)</option>
                <option value="minimax">Minimax Bot (Impossible)</option>
                <option value="dqn">DQN Bot (Trained AI)</option>
                <option value="human">Human (2 Players)</option>
            </select>
            <div id="board"></div>
            <div id="message"></div>
            <button onclick="resetGame()">Restart</button>
        </div>
        <div id="rules" style="width:250px;">
            <h3>How to Play</h3>
            <ul>
                <li><strong>Goal:</strong> Get three of your marks in a row before your opponent</li>
                <li><strong>Turn:</strong> Click an empty cell to place your mark (X for you, O for AI)</li>
                <li><strong>Win:</strong> First to get 3 marks in a line (horizontal, vertical, or diagonal)</li>
                <li><strong>Tie:</strong> If all 9 cells are filled with no winner, the game is a draw</li>
                <li><strong>AI:</strong> Choose between Random (easy) or Minimax (impossible) difficulty</li>
            </ul>

            <div class="strategy">
                <strong>Strategy Tips:</strong>
                <ul>
                    <li>Control the center - it's part of 4 different winning lines</li>
                    <li>Block your opponent when they have 2 in a row</li>
                    <li>Create forks - positions where you can win in two different ways</li>
                </ul>
            </div>
        </div>
    </div>

    <script>
        const HUMAN = 'X';
        const AI = 'O';
        const TIE = 'tie';
        const EMPTY = null;
        const winning_positions = [
            [0, 1, 2], [3, 4, 5], [6, 7, 8], // Horizontal
            [0, 3, 6], [1, 4, 7], [2, 5, 8], // Vertical
            [0, 4, 8], [2, 4, 6] // Diagonal
        ];
        let board = Array(9).fill(EMPTY);
        let gameMode = document.getElementById('gameMode').value; // Default game mode
        let currentPlayer = HUMAN; // Track whose turn it is

        const boardDiv = document.getElementById('board');
        const messageDiv = document.getElementById('message');

        // Will store [{state, action, turn}] for each move
        let gameLog = [];

        // Initialize the game board and display it
        function createBoard() {
            boardDiv.innerHTML = '';
            board.forEach((cell, idx) => {
                const cellDiv = document.createElement('div');
                cellDiv.className = 'cell';
                cellDiv.textContent = cell || '';
                // Add click event listener to each cell to handle human moves
                cellDiv.addEventListener('click', () => humanMove(idx));
                boardDiv.appendChild(cellDiv);
            });
        }

        // Handle human move
        // This function is called when a human player clicks on a cell
        // It checks if the cell is empty and if the game is still ongoing
        // If valid, it places the human's mark and checks for a winner
        // If no winner, it triggers the AI to make a move
        function humanMove(idx) {
            if (board[idx] || checkWinner(board)) return;
            let turn;

            // In human vs human mode, alternate between X and O
            if (gameMode === 'human') {
                turn = currentPlayer === HUMAN ? 1 : -1;
                board[idx] = currentPlayer;
            } else {
                // Only allow move if it's human's turn
                if (currentPlayer !== HUMAN) return;
                turn = 1;

                // Log a snapshot of the board before the move
                gameLog.push({
                    state: translateBoardToInput(board),
                    action: idx,
                    turn: turn
                });

                board[idx] = HUMAN;
            }

            createBoard();

            const result = checkWinner(board);
            if (result !== null) {
                endGame(result);
            } else {
                if (gameMode === 'human') {
                    currentPlayer = currentPlayer === HUMAN ? AI : HUMAN;
                } else {
                    setTimeout(makeAIMove, 200);
                }
            }
        }

        // Make AI move based on selected mode
        function makeAIMove() {
            if (gameMode === 'minimax') {
                aiMiniMaxMove();
            } else if (gameMode === 'dqn') {
                aiDQNMove();
            } else {
                aiRandomMove();
            }
        }

        // Update game mode when dropdown changes
        function updateGameMode() {
            gameMode = document.getElementById('gameMode').value;
            if (gameMode === 'dqn') loadDQNModel();
            resetGame();
        }


        // --- DQN AI (TensorFlow.js) ---
        let dqnModel = null;

        async function loadDQNModel() {
            if (!dqnModel) {
                try {
                    dqnModel = await tf.loadGraphModel('tfjs_model/model.json');
                } catch (e) {
                    console.error('Failed to load DQN model:', e);
                }
            }
        }

        async function aiDQNMove() {
            await loadDQNModel();
            if (!dqnModel) {
                console.warn('DQN model not loaded, falling back to random move.');
                // Fallback to random if model not loaded
                aiRandomMove();
                return;
            }
            // Prepare input: X=1, O=-1, empty=0
            const inputArr = board.map(cell => cell === HUMAN ? 1 : cell === AI ? -1 : 0);
            // Mask illegal moves
            let availableMoves = board.map((cell, idx) => cell === EMPTY ? idx : null).filter(idx => idx !== null);
            if (availableMoves.length === 0) return;
            // Run inference
            console.log('DQN Input:', inputArr);
            const inputTensor = tf.tensor2d([inputArr], [1, 9]);
            let qvals = await dqnModel.predict(inputTensor).array();
            inputTensor.dispose();
            qvals = qvals[0];
            console.log('DQN Q-values:', qvals);
            // Mask illegal moves to -Infinity
            let maskedQ = qvals.map((q, idx) => availableMoves.includes(idx) ? q : -Infinity);
            // Pick the move with the highest Q value
            let bestMove = maskedQ.indexOf(Math.max(...maskedQ));
            // Log the state before the move
            gameLog.push({
                state: translateBoardToInput(board),
                action: bestMove,
                turn: -1 // AI is O, which is -1
            });
            if (board[bestMove] === EMPTY) {
                board[bestMove] = AI;
                createBoard();
                const result = checkWinner(board);
                if (result !== null) {
                    endGame(result);
                }
            } else {
                console.warn('DQN selected an illegal move:', bestMove, '. Falling back to random move.');
                aiRandomMove();
            }
        }

        function translateBoardToInput(board) {
            return board.map(cell => cell === HUMAN ? 1 : cell === AI ? -1 : 0).slice();
        }

        function logGameToConsole() {
            // Format as Python list of tuples
            let pyList = gameLog.map(
                move => `[${JSON.stringify(move.state)}, ${move.action}, ${move.turn}]`
            ).join(",\n");
            console.log("Game log for training:\n[\n" + pyList + "\n]");
            // Optionally, clear the log for the next game
            gameLog = [];
        }

        // AI makes a random move
        // This function finds all available moves (empty cells) and randomly selects one
        // It then places the AI's mark in that cell and checks for a winner
        // If a winner is found, it ends the game
        function aiRandomMove() {
            let availableMoves = board.map((cell, idx) => cell === EMPTY ? idx : null).filter(idx => idx !== null);
            if (availableMoves.length > 0) {
                const randomMove = availableMoves[Math.floor(Math.random() * availableMoves.length)];
                board[randomMove] = AI;
                createBoard();
                const result = checkWinner(board);
                if (result !== null) {
                    endGame(result);
                }
            }
        }

        // AI makes a move using the MiniMax algorithm
        // This function evaluates all possible moves using the MiniMax algorithm
        // It finds the best move for the AI and places its mark in that cell
        // After making the move, it checks for a winner
        // If a winner is found, it ends the game
        function aiMiniMaxMove() {
            let bestScore = -Infinity;
            let move;
            for (let i = 0; i < 9; i++) {
                if (!board[i]) {
                    board[i] = AI;
                    let score = minimax(board, false);
                    board[i] = EMPTY;
                    if (score > bestScore) {
                        bestScore = score;
                        move = i;
                    }
                }
            }
            board[move] = AI;
            createBoard();
            const result = checkWinner(board);
            if (result !== null) {
                endGame(result);
            }
        }

        // MiniMax algorithm to choose the best move
        function minimax(board, isMaximizing) {
            const result = checkWinner(board);
            if (result === AI) return 1;
            if (result === HUMAN) return -1;
            if (result === TIE) return 0;

            // if no winner yet, continue the search
            // if maximizing, AI is trying to maximize its score
            // if minimizing, human is trying to minimize AI's score
            // recursively call minimax for each possible move
            // and return the best score for the current player
            if (isMaximizing) {
                let bestScore = -Infinity;
                for (let i = 0; i < 9; i++) {
                    if (!board[i]) {
                        board[i] = AI;
                        let score = minimax(board, false);
                        board[i] = EMPTY;
                        bestScore = Math.max(score, bestScore);
                    }
                }
                return bestScore;
            } else {
                let bestScore = Infinity;
                for (let i = 0; i < 9; i++) {
                    if (!board[i]) {
                        board[i] = HUMAN;
                        let score = minimax(board, true);
                        board[i] = EMPTY;
                        bestScore = Math.min(score, bestScore);
                    }
                }
                return bestScore;
            }
        }

        // Check for a winner or tie
        // This function checks all winning positions to see if there's a winner
        // If a winner is found, it returns the winner's mark (HUMAN or AI)
        // If all cells are filled and no winner, it returns TIE
        // If no winner yet, it returns null
        function checkWinner(board) {
            for (const [a, b, c] of winning_positions) {
                if (board[a] && board[a] === board[b] && board[a] === board[c]) return board[a];
            }
            if (board.every(cell => cell)) return TIE;
            return null; // No winner yet
        }

        // End the game and display the result
        function endGame(result) {
            if (result === TIE) {
                messageDiv.textContent = "It's a tie!";
            } else {
                messageDiv.textContent = `${result} wins!`;
            }
            logGameToConsole();
        }

        // Reset the game
        function resetGame() {
            board = Array(9).fill(EMPTY);
            messageDiv.textContent = '';
            createBoard();
            gameLog = [];
            currentPlayer = HUMAN;
        }

        // Initialize the game board
        createBoard();
    </script>

    <!-- Game Development Story -->
    <div class="story-container">
        <h2 class="story-section-title">The Tic-Tac-Toe Journey: From Random to AI</h2>
        <div class="story-callout">
            <strong>The Challenge:</strong> Create the simplest possible game with an AI opponent, then push it to its limits.
        </div>
        <h3 class="story-subheading">Act I: The Random Beginning</h3>
        <p>We started with what seemed like the easiest approach - a bot that randomly picks from available moves. The LLM helped us quickly scaffold the basic game structure and implement this "brainless" opponent in just a few lines of code:</p>
        <pre id="random-move-code-block" class="story-code"></pre>
        <script id="random-move-source" type="text/javascript">
            function randomMove() {
                const moves = getEmptySpaces();
                return moves[Math.floor(Math.random() * moves.length)];
            }
            document.addEventListener('DOMContentLoaded', function() {
                // Render code in code block from the script tag
                const codeBlock = document.getElementById('random-move-code-block');
                let randomMoveSource = randomMove.toString();
                // Remove leading indentation
                randomMoveSource = randomMoveSource.replace(/^ {12}/gm, '');
                if (codeBlock) codeBlock.textContent = randomMoveSource;
            });
        </script>
        <p>Surprisingly effective for testing, but after a few games, the predictable randomness became... well, predictable.</p>

        <h3 class="story-subheading">Act II: The Smartest Bot Possible</h3>
        <p>Random bot <em>wasn't</em> interesting to play. We wanted the <em>perfect</em> opponent - one that would never lose.</p>
        <p>After some research, we discovered the <strong>minimax algorithm</strong>, a decision-making strategy that looks ahead to all possible game outcomes.</p>
        <div class="story-callout story-callout-warning">
            <strong>Minimax in a Nutshell:</strong><br>
            The algorithm assumes both players play optimally. For each move, it simulates all possible futures:
            <ul>
                <li><strong>Maximizing player (AI):</strong> Picks moves that lead to the best outcome</li>
                <li><strong>Minimizing player (Human):</strong> Assumed to pick moves that minimize AI's advantage</li>
                <li><strong>Recursion:</strong> Evaluates every possible game tree branch to the end</li>
            </ul>
            Result: An unbeatable AI that either wins or draws, never loses.
            It takes at most a few seconds to compute its move, even on simple hardware.
        </div>

        <p>
            Maximum depth of the game tree is limited (9 moves), in total there are 5,478 possible game states.
            However, AI starts second, so it only needs to consider moves after the human's first turn, reducing the maximum number of states to evaluate to 5,478 / 9 = 608.
        </p>
        <p>Here is a script to calculate all possible tic-tac-toe states:</p>
        <pre id="minimax-code-block" class="story-code"></pre>
        <div style="margin: 1em 0;">
            <button id="execute-minimax-script" style="padding: 0.5em 1em; font-size: 1em;">Execute Code</button>
            <span id="minimax-script-result" style="margin-left: 1em; font-weight: bold;"></span>
        </div>
        <script id="minimax-algo-source" type="text/javascript">
            function getAllTicTacToeStates() {
                const results = new Set();

                function serialize(board) {
                    return board.join('');
                }

                function isTerminal(board) {
                    const wins = [
                        [0,1,2],[3,4,5],[6,7,8], // rows
                        [0,3,6],[1,4,7],[2,5,8], // cols
                        [0,4,8],[2,4,6]          // diags
                    ];
                    for (const [a,b,c] of wins) {
                        // Check if there're three in a row
                        if (board[a] != '-' && board[a] === board[b] && board[a] === board[c])
                            return true;
                    }
                    // If all cells are filled it's a Draw
                    return board.every(cell => cell != '-');
                }

                // Depth-first search to explore all game states
                function dfs(board, xTurn) {
                    const key = serialize(board);
                    // Avoid duplicate states
                    if (results.has(key)) return;
                    results.add(key);

                    if (isTerminal(board)) return;

                    for (let i = 0; i < 9; i++) {
                        if (board[i] == '-') {
                            board[i] = xTurn ? 'X' : 'O';
                            dfs(board, !xTurn);
                            board[i] = '-';
                        }
                    }
                }

                // Start DFS with an empty board and X's turn
                dfs(Array(9).fill('-'), true);
                return results;
            }

            document.addEventListener('DOMContentLoaded', function() {
                // Render code in code block from the script tag
                const codeBlock = document.getElementById('minimax-code-block');
                let algoSource = getAllTicTacToeStates.toString();
                // Remove leading indentation
                algoSource = algoSource.replace(/^ {12}/gm, '');
                if (codeBlock) codeBlock.textContent = algoSource;

                // Add event listener for the execute button
                const btn = document.getElementById('execute-minimax-script');
                if (btn) {
                    btn.addEventListener('click', function() {
                        const allStates = getAllTicTacToeStates();
                        const resultSpan = document.getElementById('minimax-script-result');
                        if (resultSpan) {
                            resultSpan.textContent = `Total unique states: ${allStates.size}`;
                        }
                    });
                }
            });
        </script>

        <!-- TODO: Improve text bellow -->
        <p>The LLM was invaluable here, helping us understand the algorithm and debug edge cases. But minimax had a limitation - it was <em>too perfect</em> and only worked for simple games like tic-tac-toe.</p>
        <h3 class="story-subheading">Act III: The Machine Learning Adventure</h3>
        <p>As we built more complex games, we realized we needed something more flexible than handcrafted algorithms. Could we train an AI to learn tic-tac-toe from scratch? This became our machine learning laboratory.</p>
        <h4 class="story-subheading">The Learning Method Decision Tree</h4>
        <p>We explored various machine learning approaches:</p>
        <div class="story-grid-2col">
            <div class="story-grid-box">
                <strong>Supervised Learning:</strong><br>
                Train on human expert games
                <br><em>Limited by human expertise</em>
            </div>
            <div class="story-grid-box">
                <strong>Unsupervised Learning:</strong><br>
                Find patterns in game data
                <br><em>No clear winning strategy</em>
            </div>
            <div class="story-grid-box story-grid-box-success">
                <strong>Reinforcement Learning:</strong><br>
                Learn by playing against itself
                <br><em>Can discover novel strategies</em>
            </div>
            <div class="story-grid-box">
                <strong>Genetic Algorithms:</strong><br>
                Evolve strategies over generations
                <br><em>Too slow for this problem</em>
            </div>
        </div>
        <p><strong>We chose Deep Q-Network (DQN)</strong> - a reinforcement learning approach where the AI learns by playing many games against itself, gradually understanding which moves lead to victory.</p>
        <h4 class="story-subheading">The Training Odyssey</h4>
        <p>We sketched the training script (using LLM) and started training the model.</p>
        <div class="story-callout story-callout-error">
            <strong>First Attempt: Disaster</strong><br>
            Our initial model played worse than random! It had no concept of winning or losing.
        </div>
        <p><strong>The Debugging Marathon:</strong> With the LLM as our debugging partner, we systematically tackled each issue:</p>
        <ul>
            <li><strong>Learning Rate:</strong> Too high = chaotic learning, too low = glacial progress</li>
            <li><strong>Reward Function Evolution:</strong>
                <ul>
                    <li>V1: Only reward the final outcome (+1 win, -1 loss)</li>
                    <li>V2: Reward all moves with increasing/decreasing values</li>
                    <li>V3: Heavily penalize missed winning moves (-2.0)</li>
                </ul>
            </li>
            <li><strong>Training Infrastructure:</strong> CPU vs GPU (GPU was surprisingly slow for this small problem)</li>
            <li><strong>Human Game Integration:</strong> We played against early models and fed our games back into training</li>
        </ul>
        <div class="story-callout story-callout-warning">
            <strong>The Breakthrough Moment:</strong><br>
            We discovered our training was fundamentally flawed! The model learned from random samples of past games but never guaranteed it would learn from every recent game.
            <br><br>
            <strong>The Fix:</strong> Train on the most recent game explicitly, plus random historical samples. This ensured every game contributed to learning at least once.
        </div>
        <h4 class="story-subheading">The Final Model</h4>
        <p>After countless iterations, parameter tuning, and LLM consultations, we achieved:</p>
        <div class="story-callout story-callout-success">
            <strong>&lt;10% Loss Rate</strong><br>
            Not as perfect as minimax, but learned organically through self-play. It developed its own playing style - excellent at blocking opponent moves but occasionally missing obvious wins (keeping games interesting!).
        </div>
        <h3 class="story-subheading">Human-AI Collaboration Insights</h3>
        <p>This journey revealed fascinating dynamics about working with LLMs:</p>
        <ul>
            <li><strong>Rapid Prototyping:</strong> LLMs excel at generating boilerplate code and explaining complex algorithms</li>
            <li><strong>Debugging Partner:</strong> Perfect for bouncing ideas and systematically exploring solutions</li>
            <li><strong>Knowledge Bridge:</strong> Translates academic concepts (like minimax) into practical code</li>
            <li><strong>Iteration Speed:</strong> What used to take days of research now takes hours of conversation</li>
            <li><strong>But Human Insight Remains Critical:</strong> The breakthrough training fix came from manual analysis, not AI suggestion</li>
        </ul>
        <h3 class="story-subheading">Future Possibilities</h3>
        <p>We might revisit this model to achieve 0% loss rate, or explore other learning algorithms. But the real victory was the journey - from random moves to self-learning AI, with an LLM as our tireless coding companion.</p>
        <div class="story-callout story-callout-center">
            <strong>Try all three AI modes above and see the evolution for yourself!</strong><br>
            <em>Random → Minimax → DQN - each represents a different chapter in AI development</em>
        </div>
    </div>
</body>

</html>